{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ResumeParser('Ahtasham.pdf').get_extracted_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdffileobj=open('Rafay-Mahmood-ML-DS.pdf','rb')\n",
    " \n",
    "pdfreader=PyPDF2.PdfFileReader(pdffileobj)\n",
    " \n",
    "x=pdfreader.numPages\n",
    " \n",
    "pageobj=pdfreader.getPage(x+1)\n",
    " \n",
    "text=pageobj.extractText()\n",
    "text\n",
    "# file1=open(r\"C:\\Users\\SIDDHI\\AppData\\Local\\Programs\\Python\\Python38\\\\1.txt\",\"a\")\n",
    "# file1.writelines(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "reader = PdfReader(\"Ahtasham.pdf\")\n",
    "number_of_pages = len(reader.pages)\n",
    "page = reader.pages[0]\n",
    "text = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyresparser import ResumeParser\n",
    "import os\n",
    "from docx import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import nltk\n",
    " \n",
    "nltk.download('stopwords')\n",
    " \n",
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "]\n",
    " \n",
    " \n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    " \n",
    " \n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    " \n",
    "    # remove the stop words\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    " \n",
    "    # remove the punctuation\n",
    "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    # generate bigrams and trigrams (such as artificial intelligence)\n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    " \n",
    "    # we create a set to keep the results in.\n",
    "    found_skills = set()\n",
    " \n",
    "    # we search for each token in our skills database\n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    " \n",
    "    # we search for each bigram and trigram in our skills database\n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('Ahtasham.pdf')\n",
    "    skills = extract_skills(text)\n",
    " \n",
    "    print(skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    " \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    " \n",
    "# you may read the database from a csv file or some other database\n",
    "SKILLS_DB = [\n",
    "    'machine learning',\n",
    "    'data science',\n",
    "    'python',\n",
    "    'word',\n",
    "    'excel',\n",
    "    'English',\n",
    "    'ASP.Net CORE',\n",
    "    'sql',\n",
    "    'html ',\n",
    "    'web',\n",
    "    'api',\n",
    "]\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.tokenize.word_tokenize(input_text)\n",
    " \n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\n",
    "    filtered_tokens = [w.lower() for w in word_tokens if w.isalpha()]\n",
    " \n",
    "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\n",
    "    found_skills = set()\n",
    " \n",
    "    for token in filtered_tokens:\n",
    "        if token.lower() in SKILLS_DB:\n",
    "            found_skills.add(token)\n",
    "            \n",
    "    for ngram in bigrams_trigrams:\n",
    "        if ngram.lower() in SKILLS_DB:\n",
    "            found_skills.add(ngram)\n",
    " \n",
    "    return found_skills\n",
    "def extract_names(input_text):\n",
    "    person_names = []\n",
    " \n",
    "    for sent in nltk.sent_tokenize(input_text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\n",
    "                person_names.append(\n",
    "                    ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\n",
    "                )\n",
    " \n",
    "    return person_names\n",
    "\n",
    "\n",
    "def extract_emails(input_text):\n",
    "    EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "    return re.findall(EMAIL_REG, input_text)\n",
    "\n",
    "def extract_phone_number(input_text):\n",
    "#     PHONE_REG = re.compile(r'[\\+\\(]?[0-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    PHONE_REG = re.compile(r'(?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n",
    "    \n",
    "    \n",
    "    phone = re.findall(PHONE_REG, input_text)\n",
    "    if phone:\n",
    "        return phone\n",
    "    return None\n",
    "\n",
    "def extract_education(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    EDUCATION = [\n",
    "            'BE','B.E.', 'B.E', 'BS-IT', 'B.S','C.A.','c.a.','B.Com','B. Com','M. Com', 'M.Com','M. Com .','F.S.C','BS'\n",
    "            'ME', 'M.E', 'M.E.', 'MS', 'M.S',\n",
    "            'BTECH', 'B.TECH', 'M.TECH', 'MTECH',\n",
    "            'PHD', 'phd', 'ph.d', 'Ph.D.','MBA','mba','graduate', 'post-graduate','5 year integrated masters','masters',\n",
    "            'SSC', 'HSC', 'CBSE', 'ICSE'\n",
    "        ]\n",
    "    nlp_text = nlp(input_text)\n",
    "    nlp_text = [sent.text.strip() for sent in nlp_text.sents]\n",
    "    edu = {}\n",
    "    for index, text in enumerate(nlp_text):\n",
    "        for tex in text.split():\n",
    "            tex = re.sub(r'[?|$|.|!|,]', r'', tex)\n",
    "            if tex.upper() in EDUCATION and tex not in stop_words:\n",
    "                edu[tex] = text + nlp_text[index + 1]\n",
    "                return list(edu.keys())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = extract_text_from_pdf('Rafay-Mahmood-ML-DS.pdf').lower()\n",
    "    skills = extract_skills(text)\n",
    "    names = extract_names(text)\n",
    "    emails = extract_emails(text)\n",
    "    number = extract_phone_number(text)\n",
    "    education = extract_education(text)\n",
    "    \n",
    "    \n",
    "    print(skills)\n",
    "    print(emails)   \n",
    "    print(number)   \n",
    "    print(education)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Donad Trump was President of USA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Donad Trump was President of USA"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Donad Trump, USA)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Donad Trump, spacy.tokens.span.Span)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.ents[0], type(doc.ents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Donad Trump\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was President of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    USA\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# with open('Entity Recognition in Resumes.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "data = [json.loads(line) for line in open('Entity Recognition in Resumes.json', 'r',encoding=\"utf-8\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Akhil Yadav Polemaina\\nHyderabad, Telangana - Email me on Indeed: indeed.com/r/Akhil-Yadav-Polemaina/\\nf6931801c51c63b1\\n\\n● Senior System Engineer at Infosys with 3.2 years of experience in software development and\\nMaintenance.\\n● Maintained data processing using mainframe technology for multiple front end applications of\\nWalmart Retail Link platform and ensured on-time deliverables.\\n● Worked on automating the uses cases to reduce manual effort in solving repeating incidents\\nusing Service Now orchestration.\\n● Possess good analytical, logical ability and systematic approach to problem analysis, strong\\ndebugging and troubleshooting skills.\\n● Good exposure to Retail domain.\\n\\nWilling to relocate to: hyderbad, Telangana\\n\\nWORK EXPERIENCE\\n\\nSenior Systems Engineer\\n\\nInfosys Limited -  Hyderabad, Telangana -\\n\\nJanuary 2015 to Present\\n\\n● Working on all the Major and Minor Enhancement requests as part of Maintenance and Support\\nactivities\\n● Identifying and fixing all the major defects in the applications, perform root cause analysis for\\nproduction issues\\n● Being a subject matter expert, involved in multiple Knowledge transfer and knowledge sharing\\nsessions with the client\\n● Leading a peer group and taking end to end responsibilities for all the critical issues/\\nenhancements.\\n● Identifying the use cases which can be automated using Service Now Orchestration\\n● Creating workflows to automate various tasks which involved manual intervention\\n● Direct interaction with the client on various business impacting issues on a daily basis\\n● Setting up Weekly Status Review meetings and Code Review meetings with the client\\n\\nSenior Systems Engineer\\n\\nInfosys Limited -  Hyderabad, Telangana -\\n\\nJanuary 2015 to Present\\n\\nTeam Size # 5\\nProject Objective:\\nProviding end to end Maintenance and Support activity for data processing of the most critical and\\nimportant Web portal 'Retail Link' along with over 40 applications used daily by all the Suppliers\\nand Business users of Walmart, the largest retailer in the world. Retail link is a portal which hosts\\n100's of applications developed across technologies for the suppliers which help them to carry\\non day-to-day activities right from on boarding to tracking their sales. This involves supporting\\n\\nhttps://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-download&ikw=download-top&co=IN\\nhttps://www.indeed.com/r/Akhil-Yadav-Polemaina/f6931801c51c63b1?isid=rex-download&ikw=download-top&co=IN\\n\\n\\nvarious Decision Support System reports which helps the higher management to take business\\ncritical decisions.\\n\\nResponsibilities:\\n● Working on all the Major and Minor Enhancement requests as part of Maintenance and Support\\nactivities\\n● Identifying and fixing all the major defects in the applications, perform root cause analysis for\\nproduction issues\\n● Being a subject matter expert, involved in multiple Knowledge transfer and knowledge sharing\\nsessions with the client\\n● Leading a peer group and taking end to end responsibilities for all the critical issues/\\nenhancements.\\n● Identifying the use cases which can be automated using Service Now Orchestration\\n● Creating workflows to automate various tasks which involved manual intervention\\n● Direct interaction with the client on various business impacting issues on a daily basis\\n● Setting up Weekly Status Review meetings and Code Review meetings with the client\\n\\nEDUCATION\\n\\nElectrical and Electronics Engineering\\n\\nAnurag College of Engineering (Jntuh)\\n\\nSKILLS\\n\\nservicenow (1 year), Mainframe (3 years), cobol (3 years), Jcl (3 years), Teradata (3 years)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n• Domain - Retail\\n• Technology - Mainframe (COBOL, JCL, DB2, Teradata), Service now.\\n• Operating System - Mainframe (z/OS)\\n• Database - DB2, SQL, Teradata.\\n• Utilities - FILE-AID, IDCAMS, DFSORT basics, LIBRARIAN, FTP/SFTP, CA-7 basics.\\n• Tools - Query Management Tool (QMF), SQL Assistant, Service now, Remedy.\\n\\nKey Strengths:\\n● Effective Communication Skills and Zeal to learn.\\n● Flexibility and Adaptability.\\n● Good Leadership Qualities.\\n● Analytical and Problem Solving Skills.\\n\\nAchievements:\\n● Received STAR award for working on various system improvement and automation activities\\n● Received multiple INSTA awards for my performance in the projects worked\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': ['Degree'],\n",
       "  'points': [{'start': 7878, 'end': 7882, 'text': 'B.B.M'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 5840, 'end': 5846, 'text': 'Infosys'}]},\n",
       " {'label': ['Email Address'],\n",
       "  'points': [{'start': 2090,\n",
       "    'end': 2136,\n",
       "    'text': 'indeed.com/r/Debasish-Dasgupta/a20561e10f83ae3f'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 1471, 'end': 1477, 'text': 'Infosys'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 1432,\n",
       "    'end': 1469,\n",
       "    'text': 'Trainer-Finacle-Core Banking Solutions'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 203, 'end': 209, 'text': 'Infosys'}]},\n",
       " {'label': ['Email Address'],\n",
       "  'points': [{'start': 128,\n",
       "    'end': 174,\n",
       "    'text': 'indeed.com/r/Debasish-Dasgupta/a20561e10f83ae3f'}]},\n",
       " {'label': ['Location'],\n",
       "  'points': [{'start': 93, 'end': 97, 'text': 'Qasba'}]},\n",
       " {'label': ['Companies worked at'],\n",
       "  'points': [{'start': 57, 'end': 63, 'text': 'Infosys'}]},\n",
       " {'label': ['Designation'],\n",
       "  'points': [{'start': 18,\n",
       "    'end': 55,\n",
       "    'text': 'Trainer-Finacle-Core Banking Solutions'}]},\n",
       " {'label': ['Name'],\n",
       "  'points': [{'start': 0, 'end': 16, 'text': 'Debasish Dasgupta'}]}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[167]['annotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\") # load a new spacy model\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for i in range(0,len(data)):\n",
    "    temp_dict = {}\n",
    "    temp_dict['text'] = data[i]['content']\n",
    "    temp_dict['entities'] = []\n",
    "    for j in range(0,len(data[i]['annotation'])):\n",
    "        start = data[i]['annotation'][j]['points'][0]['start']\n",
    "        end = data[i]['annotation'][j]['points'][0]['end']\n",
    "#         print(i,j)\n",
    "        label = data[i]['annotation'][j]['label'][0].upper()\n",
    "        \n",
    "        \n",
    "        temp_dict['entities'].append((start, end, label))\n",
    "    training_data.append(temp_dict)\n",
    "    \n",
    "# print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# training_data = []\n",
    "# for i in range(0,len(data)):\n",
    "#     temp_dict = {}\n",
    "#     temp_dict['text'] = data[i][0]\n",
    "#     temp_dict['entities'] = []\n",
    "#     for j in range(0,len(data[i][1]['entities'])):\n",
    "#         new = tuple(data[i][1]['entities'][j])\n",
    "#         temp_dict['entities'].append(new)\n",
    "        \n",
    "#     training_data.append(temp_dict)\n",
    "\n",
    "# print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Abhishek Jha\\nApplication Development Associate - Accenture\\n\\nBengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a\\n\\n• To work for an organization which provides me the opportunity to improve my skills\\nand knowledge for my individual and company's growth in best possible ways.\\n\\nWilling to relocate to: Bangalore, Karnataka\\n\\nWORK EXPERIENCE\\n\\nApplication Development Associate\\n\\nAccenture -\\n\\nNovember 2017 to Present\\n\\nRole: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries\\nfor the Bot which will be triggered based on given input. Also, Training the bot for different possible\\nutterances (Both positive and negative), which will be given as\\ninput by the user.\\n\\nEDUCATION\\n\\nB.E in Information science and engineering\\n\\nB.v.b college of engineering and technology -  Hubli, Karnataka\\n\\nAugust 2013 to June 2017\\n\\n12th in Mathematics\\n\\nWoodbine modern school\\n\\nApril 2011 to March 2013\\n\\n10th\\n\\nKendriya Vidyalaya\\n\\nApril 2001 to March 2011\\n\\nSKILLS\\n\\nC (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year),\\nDatabase Management System (Less than 1 year), Java (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTechnical Skills\\n\\nhttps://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Programming language: C, C++, Java\\n• Oracle PeopleSoft\\n• Internet Of Things\\n• Machine Learning\\n• Database Management System\\n• Computer Networks\\n• Operating System worked on: Linux, Windows, Mac\\n\\nNon - Technical Skills\\n\\n• Honest and Hard-Working\\n• Tolerant and Flexible to Different Situations\\n• Polite and Calm\\n• Team-Player\", 'entities': [(1295, 1621, 'SKILLS'), (993, 1153, 'SKILLS'), (939, 956, 'COLLEGE NAME'), (883, 904, 'COLLEGE NAME'), (856, 860, 'GRADUATION YEAR'), (771, 813, 'COLLEGE NAME'), (727, 769, 'DESIGNATION'), (407, 415, 'COMPANIES WORKED AT'), (372, 404, 'DESIGNATION'), (95, 145, 'EMAIL ADDRESS'), (60, 68, 'LOCATION'), (49, 57, 'COMPANIES WORKED AT'), (13, 45, 'DESIGNATION'), (0, 11, 'NAME')]}\n"
     ]
    }
   ],
   "source": [
    "print(training_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 24/220 [00:00<00:01, 108.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 51/220 [00:00<00:01, 116.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 75/220 [00:00<00:01, 111.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 121/220 [00:01<00:00, 103.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 161/220 [00:01<00:00, 146.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 208/220 [00:01<00:00, 137.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:01<00:00, 123.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n",
      "Skipping entity\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example  in tqdm(training_data): \n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"train.spacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Auto-filled config with all values\n",
      "[+] Saved config\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Saving to output directory: .\n",
      "[i] Using CPU\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[+] Initialized pipeline\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "[i] Pipeline: ['tok2vec', 'ner']\n",
      "[i] Initial learn rate: 0.001\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "[!] Aborting and saving the final best model. Encountered exception:\n",
      "OSError('154247100 requested and 0 written')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-03-28 11:10:18,004] [INFO] Set up nlp object from config\n",
      "[2023-03-28 11:10:18,013] [INFO] Pipeline: ['tok2vec', 'ner']\n",
      "[2023-03-28 11:10:18,016] [INFO] Created vocabulary\n",
      "[2023-03-28 11:10:19,179] [INFO] Added vectors: en_core_web_lg\n",
      "[2023-03-28 11:10:20,204] [INFO] Finished initializing nlp object\n",
      "[2023-03-28 11:10:29,893] [INFO] Initialized pipeline components: ['tok2vec', 'ner']\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 124, in train\n",
      "    raise e\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 112, in train\n",
      "    save_checkpoint(is_best_checkpoint)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 68, in save_checkpoint\n",
      "    before_to_disk(nlp).to_disk(output_path / DIR_MODEL_LAST)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\language.py\", line 2036, in to_disk\n",
      "    util.to_disk(path, serializers, exclude)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1356, in to_disk\n",
      "    writer(path / key)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\language.py\", line 2035, in <lambda>\n",
      "    serializers[\"vocab\"] = lambda p: self.vocab.to_disk(p, exclude=exclude)\n",
      "  File \"spacy\\vocab.pyx\", line 472, in spacy.vocab.Vocab.to_disk\n",
      "  File \"spacy\\vectors.pyx\", line 587, in spacy.vectors.Vectors.to_disk\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1356, in to_disk\n",
      "    writer(path / key)\n",
      "  File \"spacy\\vectors.pyx\", line 583, in spacy.vectors.Vectors.to_disk.lambda5\n",
      "  File \"spacy\\vectors.pyx\", line 578, in spacy.vectors.Vectors.to_disk.save_vectors\n",
      "  File \"spacy\\vectors.pyx\", line 579, in spacy.vectors.Vectors.to_disk.save_vectors\n",
      "  File \"spacy\\vectors.pyx\", line 569, in spacy.vectors.Vectors.to_disk.lambda2\n",
      "  File \"<__array_function__ internals>\", line 5, in save\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 529, in save\n",
      "    format.write_array(fid, arr, allow_pickle=allow_pickle,\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\", line 691, in write_array\n",
      "    array.tofile(fp)\n",
      "OSError: 154247100 requested and 0 written\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\__main__.py\", line 4, in <module>\n",
      "    setup_cli()\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\cli\\_util.py\", line 74, in setup_cli\n",
      "    command(prog_name=COMMAND)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1128, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\typer\\core.py\", line 778, in main\n",
      "    return _main(\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\typer\\core.py\", line 216, in _main\n",
      "    rv = self.invoke(ctx)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1659, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\click\\core.py\", line 1395, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\click\\core.py\", line 754, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\typer\\main.py\", line 683, in wrapper\n",
      "    return callback(**use_params)  # type: ignore\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\cli\\train.py\", line 45, in train_cli\n",
      "    train(config_path, output_path, use_gpu=use_gpu, overrides=overrides)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\cli\\train.py\", line 75, in train\n",
      "    train_nlp(nlp, output_path, use_gpu=use_gpu, stdout=sys.stdout, stderr=sys.stderr)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 128, in train\n",
      "    save_checkpoint(False)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\training\\loop.py\", line 68, in save_checkpoint\n",
      "    before_to_disk(nlp).to_disk(output_path / DIR_MODEL_LAST)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\language.py\", line 2036, in to_disk\n",
      "    util.to_disk(path, serializers, exclude)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1356, in to_disk\n",
      "    writer(path / key)\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\language.py\", line 2035, in <lambda>\n",
      "    serializers[\"vocab\"] = lambda p: self.vocab.to_disk(p, exclude=exclude)\n",
      "  File \"spacy\\vocab.pyx\", line 472, in spacy.vocab.Vocab.to_disk\n",
      "  File \"spacy\\vectors.pyx\", line 587, in spacy.vectors.Vectors.to_disk\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\spacy\\util.py\", line 1356, in to_disk\n",
      "    writer(path / key)\n",
      "  File \"spacy\\vectors.pyx\", line 583, in spacy.vectors.Vectors.to_disk.lambda5\n",
      "  File \"spacy\\vectors.pyx\", line 578, in spacy.vectors.Vectors.to_disk.save_vectors\n",
      "  File \"spacy\\vectors.pyx\", line 579, in spacy.vectors.Vectors.to_disk.save_vectors\n",
      "  File \"spacy\\vectors.pyx\", line 569, in spacy.vectors.Vectors.to_disk.lambda2\n",
      "  File \"<__array_function__ internals>\", line 5, in save\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\", line 529, in save\n",
      "    format.write_array(fid, arr, allow_pickle=allow_pickle,\n",
      "  File \"C:\\Users\\Rafay\\anaconda3\\lib\\site-packages\\numpy\\lib\\format.py\", line 691, in write_array\n",
      "    array.tofile(fp)\n",
      "OSError: 154247100 requested and 0 written\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./train.spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ner = spacy.load(\"model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = '''Jatin Arora\\nSDET Automation Engineer, Infosys - CRD (Capital Group of Companies)\\n\\nPehowa, Haryana - Email me on Indeed: indeed.com/r/Jatin-Arora/a124b9609f62fbcb\\n\\n3.6 years of experience in Automation Testing tools like Selenium (C#), Java, dot Net Technology\\nand other Testing tools like JIRA, QC. Proficient in testing on Client/Server and Web-based with\\ngood experience in creating the Test Plan and testing framework\\n\\n• Experience in SQA including Selenium with data driven framework.\\n• Experience in working on C# and dot net technologies.\\n• Automated the Test data creation using Selenium as value added service for a client which is\\nbeing used by the client and bought 30% cost reduction in data setup.\\n• Developed Test Case Generator as value added services for client which reduced time and cost\\nby\\n45% being spent on data creation and provide more productivity.\\n• Developed an Infosys Go Safe Android App during participation in Global Hackathon\\n• Excellent communications skills and able to liaise with key customer contacts and vendors to\\nresolve software issues.\\n• Having excellent problem solving and analytical skills\\n\\nCompetencies Developed:\\n\\nSelenium WebDriver C# ADO.Net SQL HTML, CSS\\nJava Unix Quality Center Atlassian JIRA MTM\\nMicrosoft SQL Server Visual Studio Rapid SQL Toad Confluence\\nEclipse Visual Studio BitBucket NUnit MindMaps\\n\\nTesting Skills:\\nSDLC, STLC, Test Planning, Requirement Analysis, Agile Methodology, DevOps Methodology,\\nScrum, Waterfall, Software Quality, Defect Tracking, Regression Testing, System Testing\\n\\nWORK EXPERIENCE\\n\\nSDET Automation Engineer, Infosys\\n\\nCRD (Capital Group of Companies) -  Chandigarh, Chandigarh -\\n\\nFebruary 2017 to Present\\n\\nCRD (Charles River Development IMS automates the compliance workflow and provides\\ncentralized compliance monitoring and management. The highly scalable compliance engine\\nsupports very high volumes of trades, compliance rules, accounts and group of accounts.\\n\\n• Design and implemented test scenarios, test cases, QA processes and procedures.\\n• Automated and delivered high quality automation scripts using C#.\\n• Writing SQL queries to validate data from database.\\n• Used BitBucket for code repository.\\n• Communication with stakeholder for business and functional requirements\\n\\nhttps://www.indeed.com/r/Jatin-Arora/a124b9609f62fbcb?isid=rex-download&ikw=download-top&co=IN\\n\\n\\n• Perform requirements analysis and impact analysis for enhancements and changes\\n• Creating weekly Test Unit creation and execution report\\n\\nInfosys Training, Test Engineer Trainee\\n\\nNext Generation Volume Licensing (Microsoft) -  Mysore, Karnataka -\\n\\nAugust 2014 to November 2014\\n\\nA detailed hands on training of C# and SQL concepts and testing training.\\n\\nTest Engineer, Infosys\\n\\nNext Generation Volume Licensing (Microsoft) -  Hyderabad, Telangana -\\n\\nApril 2014 to November 2014\\n\\nJan 2017\\n\\nProject Next Generation Volume Licensing (NGVL) is a strategic change initiative led by\\nWorld Wide Licensing &amp; Pricing (WWLP), Operations, MSIT, and Worldwide Operations in\\npartnership with EPG, SMS&amp;P, and WPG to transform the Volume Licensing business so that\\nit can scale for the next 20 years, compete more quickly in new markets, and deliver a superior\\nlicensing experience for our customers, partners, sellers, and operations personnel.\\n\\n• Designed and implemented test scenarios, test cases, QA processes and procedures.\\n• Automated and delivered high quality automation scripts with Selenium using C#.\\n• Preparing SQLs for validating data in database.\\n• Communicated with stakeholder for business and functional requirements\\n• Performed requirement analysis and impact analysis for enhancements and changes\\n• Logging and tracking defects and preparing backlog report\\n• Created weekly Test Unit creation and execution report\\n• Performed peer reviews and estimates which helped to automate 95% web site automation\\n\\nEDUCATION\\n\\nB.Sc in Computer Science\\n\\nKurukshetra University -  Kurukshetra, Haryana\\n\\nTagore Public School -  Pehowa, Haryana\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp_ner(asd)\n",
    "\n",
    "colors = {\"PATHOGEN\": \"#F67DE3\", \"MEDICINE\": \"#7DF6D9\", \"MEDICALCONDITION\":\"#a6e22d\"}\n",
    "options = {\"colors\": colors} \n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\", options= options, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for filename in os.listdir(os.getcwd()):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r') as f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "abs_path.strip()\n",
    "for i in range(1,36):\n",
    "    path = r\"C:\\Users\\Rafay\\Desktop\\Newfolder\\ \"+ str(i) + \".pdf\"\n",
    "    path = path.replace(\" \",\"\")\n",
    "    text = extract_text(path)\n",
    "    \n",
    "    path = r\"C:\\Users\\Rafay\\Desktop\\Newfolder\\ \"+ str(i) + \".txt\"\n",
    "    path = path.replace(\" \",\"\")\n",
    "    \n",
    "    f = open(path, \"w\",encoding=\"utf-8\")\n",
    "    f.write(text.lower())\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
